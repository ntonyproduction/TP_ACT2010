{
    "collab_server" : "",
    "contents" : "---\ntitle: \"\"\nauthor:\n- \noutput: \n  pdf_document: \n    toc: yes\n    number_sections: False\ndocumentclass: article\nlang: fr  \n---\n\n\\newpage\n\n# Numéro 1\n## a)\nTout d'abord, nous observons la série chronologique suivante concernant le taux de change américain/européen depuis janvier 1999 jusqu'à décembre 2016. Les données ont été collectées de façon quotidienne pour ensuite être transformées mensuellement. Voici le résultat:\n\n\n```{r graphique_A, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, fig.height=3.3}\nlibrary('TSA')\nlibrary('tseries')\n\n# Importation des données\ntaux <- read.csv2(\"C:/Users/ANGAG426/Desktop/TP_ACT2010/Taux_de_change_US_Euro.csv\")\nrendement<-taux$US.Euro\nanne.mois<-taux$Année.mois\n\n# création de la série chronologique\nttaux<-ts(rendement,start=c(1999,1),end=c(2016,12),frequency = 12)\n\n# graphique de la série\nplot(ttaux,ylab='USD/Euro', xlab=\"Temps\", main=\"Figure 1: taux de change US/Euro\", type=\"o\", cex=0.5)\nabline(h=1)\n```\n\n## b)\nGrâce à une première analyse, on remarque sur le tableau de la fonction d'autocorrélation échantillonale (ACF) qu'il y a une forte présence d'autocorrélation et que celle-ci diminue lentement plus le lag augmente (voir figure 2). On peut donc déduire que le processus est non-stationnaire. Il n'est pas nécessaire de poursuivre notre analyse avec le graphique de la fonction d'autocorrélation partielle (PACF) puisque nous devrons effectuer une différentiation sur le modéle pour créer l'effet de stationnarité.\n\n```{r acf, eval=TRUE, echo=FALSE,fig.height=3.5}\nacf(ttaux, main=\"Figure 2: fonction d'autocorrélation\")\n```\n\n## c)\nÉtant donné que les données sont positives, Il est possible d'utiliser Box-Cox afin de transformer notre processus. En ce faisant, la variance diminuera. On rappelle que la famille des fonctions de puissance est définie de la façon suivante:\n\n$$\n\\begin{aligned}\ng(x) = \\frac{x^{\\lambda}-1}{\\lambda} \\times 1_{\\{\\lambda \\neq 0 \\}} + \\text{ln}(x) \\times 1_{\\{\\lambda = 0 \\}}\n\\end{aligned}\n$$\n\n$\\lambda$ est donc déterminé en maximisant la fonction de log-vraisemblance de nos données que voici:\n\n```{r BoxCox, eval=T, echo=F, fig.height=3.3, message=FALSE, warning=FALSE}\nBoxCox.ar(ttaux)\n```\n\nOn constate que $\\lambda = 0.1$ semble étre l'estimé MLE situé au centre de l'intervalle de confiance 95%, soit $]-0.5, 0.7[$. Puisque $\\lambda = 0$ est dans notre IC, cette valeur du paramètre peut également étre à considérer. Donc, on utilise la transformé logarithmique pour notre modèle (voir figure 3).\n\n```{r log_processus, eval=T, echo=F, fig.height=3.3}\nplot(log(ttaux), type='o', ylab='log', main=\"Figure 3: logarithme du taux\", cex=0.5)\nabline(h=0)\n```\n\n\nCependant, en analysant le graphique de la fonction d'autocorrélation du logarithme de  notre série, on remarque qu'il n'y a pas de variation notable. Le modèle est toujours non stationnaire. PLUGGER LA THéORIE DE YAN SUR LE MFE PI LES SHIT DE RENDEMENT.\n\n## d)\nEn revanche, si on effectue une première différentiation du log de notre série, on obtient la série de la figure 4 qui semble plus stable.\n\n```{r diff_log, eval=T, echo=F, fig.height=3.3}\nplot(diff(log(ttaux)), type='o', ylab='différencitation du logarithme', main = \"Figure 4: différencitation du logarithme\")\n```\n\nOn vérifie alors si une première différentiation amène la stationnarité à notre processus en utilisant le test augmenté de Dickey-Fuller (ADF). En ce concentrant sur la partie autorégressive, on obtient un processus AR(1) associé à notre modèle de la première différenciation du logarithme. Par la suite, on applique le test ADF à 95% et on obtient comme estimé $a/\\sigma_a$, $t_{DF} = -9.5181$, avec une p-value de 0.01. Le test semble corroborer que la série chronologique est stationnaire, i.e qu'une première différenciation doit s'appliquer.\n\n## e)\nOn cherche maintenant le modèle de notre série stationnaire. En observant le graphique ACF et PACF de la figure 5 et 6. Il est à noter que le lag est exprimé en année, ce qui veut dire que 0.1 équivaut à 1/10 d'année et que 0.5 équivaut à 6 mois.\n\n```{r, eval=T, echo=F, fig.height=3.3}\nacf(diff(log(ttaux)), main=\"Figure 5: ACF de la différence du logarithme\")\npacf(diff(log(ttaux)), main=\"Figure 6: PACF de la différence du logarithme\")\n```\n\nIl semblerait que l'on soit en présence d'un ARIMA(1,1,0) en se fiant au tableau ACF. Pour ce qui est du graphique PACF, on observe une ARIMA(0,1,1). Nous confirmons le tout avec la fonction d'autocorrélation étendue (EACF) ci-dessous.\n\n```{r, eval=T, echo=F, fig.height=3.3}\neacf(diff(log(ttaux)))\n```\n\nCe graphique porte à croire que l'on est en présence d'un modèle ARIMA(0,1,1), i.e. IMA(1). On continue notre analyse en appliquant le critère d'information Bayesienne (BIC).\n\n```{r, eval=T, echo=F, fig.height=3.3}\nplot(armasubsets(y=diff(log(ttaux)), nar=3, nma=3, y.name='test',ar.method='ols'))\n```\n\nNous concluons à partir de ce graphique que le meilleur modèle à considérer est l'ARIMA(0,1,3).\n\nPour résumé, après avoir analyser les différents graphiques de corrélation et les différents tests, nous considérerons les 3 modèles suivants basés sur le logarithme:\n\n* ARIMA(1,1,0);\n* ARIMA(0,1,1);\n* ARIMA(0,1,3).\n\n## f)\nPar la suite, il faut estimer les paramètres de chacun des modèles énumérés en (e) par la méthode du maximum de vraisemblance (MLE).\n\n```{r MLE_ARIMA011, eval=T, echo=F}\nttaux011 <- arima(log(ttaux), order=c(0,1,1), method='ML')\n```\n\nPour l'ARIMA(0,1,1), le paramètre $\\theta_1$ de notre MA(1) est de 0.3118, ce qui nous donne le modèle suivant:\n$$\nlog Y_t = log Y_{t-1} + e_t + 0.3118e_{t-1}\n$$\n```{r MLE_ARIMA110, eval=T, echo=F}\nttaux110 <- arima(log(ttaux), order=c(1,1,0), method='ML')\n```\n\nPour l'ARIMA(1,1,0), le paramètre $\\phi_1$ de notre AR(1) est de 0.2933, ce qui nous donne le modèle suivante:\n\n$$\nlog Y_t = 1,2933\\ log Y_{t-1} - 0.2933\\ logY_{t-2} + e_t\n$$\nPour l'ARIMA(0,1,3), le paramètre $\\phi_1$ de notre AR(1) est de 0.2933, ce qui nous donne le modèle suivante:\n# voir chapitre 5 pour écrire le modèle\n\n\\newpage\n\n# Annexe\n\n```{r ref1, ref.label=\"graphique_A\", echo=TRUE, eval=FALSE}\n\n```\n```{r ADF, eval=F, echo=T}\n# Test augmenté de Dickey-Fuller (ADF)\nar(log(ttaux))\nadf.test(log(ttaux), k=2)\n```\n```{r ref2, ref.label=\"MLE_ARIMA011\", eval=F, echo=T}\n\n```\n\n",
    "created" : 1511327486443.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2598505167",
    "id" : "4FA2A872",
    "lastKnownWriteTime" : 1511327441,
    "last_content_update" : 1511327441,
    "path" : "C:/Users/ANGAG426/Desktop/TP_ACT2010/tp_act2010.Rmd",
    "project_path" : "tp_act2010.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}