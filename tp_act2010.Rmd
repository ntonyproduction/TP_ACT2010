---
title: ""
author:
- 
output: 
  pdf_document: 
    toc: yes
    number_sections: False
documentclass: article
lang: fr  
---

\newpage

#Introduction au rapport

Le présent rapport se veut être l'analyse de différentes séries chronologiques. Plus précisément, ce rapport a pour but de trouver le modèle qui s'ajuste le plus exactement à nos échantillons de données. Le travail en bref sera alors de séparer nos bases de données en un échantillons \textit{entraînement} qui servira à trouver un modèle et en un échantillon \textit{test} qui servira à valider la précision du modèle retenu. Le travail est fait à partir de deux bases de données pour un total de cinq variables à modéliser. La première base de données, traitée au numéro 1, contient les données mensuelles du taux de change du dollar américain par rapport à la monnaie Euro de janvier 1999 à décembre 2016. La seconde base de données contient une séries de statistiques de la \textit{SAAQ}, soit le nombre d'accidents automobiles avec dommages corporels, le nombre de personnes accidentés, le nombre de demandes d'indemnités et le coût total de lindemnisation (en dollar constants 2015) pour les années 1978 à 2015 inclusivement. Ces données seront traîtées au numéro 2. Afin d'alléger la lecture du présent rapport, les figures et tableaux furent placés en annexe.

\newpage

#No. 1 Taux de change US/Euro
## Analyse primaire du jeu de données

```{r no1.1, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
library('TSA')
library('tseries')

# Importation des données
taux <- read.csv2("C:/Users/TEMP.ULAVAL/Documents/GitHub/TP_ACT2010/Taux_de_change_US_Euro.csv")
rendement<-taux$US.Euro
anne.mois<-taux$Année.mois

# création de la série chronologique
ttaux<-ts(rendement,start=c(1999,1),end=c(2016,12),frequency = 12)
```

Tout d'abord, nous observons, à la \textit{Figure 1} en annexe, la série chronologique du taux de change américain/européen depuis janvier 1999 jusqu'à décembre 2016. Les données ont été collectées de façon quotidienne pour ensuite être transformées en taux mensuelles. 
\newline\newline
Grâce à une première analyse, on remarque sur le tableau de la fonction d'autocorrélation échantillonale qu'il y a une forte autocorrélation dans notre jeu de données et que celle-ci diminue lentement plus le lag augmente (voir \textit{Figure 2}). On soupçonne donc fortement ce processus d'être non-stationnaire. Il n'est pas nécessaire de tracer le graphique de la fonction d'autocorrélation partielle puisque on doit d'abord stationnarisé notre processus avant de réellement débuter son analyse.

##Test de stationnarité

```{r no1.2, echo=F}
#test augmente de Dickey-Fuller
#ar(ttaux)
ttaux.adf<-adf.test(ttaux, k=ar(ttaux)$order)
```
Le test de stationnarité de Dickey-Fuller est alors effectué sur notre base de données. On remarque que pour un ordre $k=2$ de processus auto-autoregressif tel que proposé par la fonction \textit{ar} de \textit{R}, notre processus est non stationnaire à un niveau de confiance de $5$% selon ce test avec une \textit{p-value} très forte de `r round(ttaux.adf$p.value,4)*100`%. On vérifie alors si une transformée Box-Cox est apppropriée à notre modèle avant de tester la stationnarité d'une première différentiation. 

##Transformation des données
```{r no1.3, echo=F,eval=F, warning=F}
#valeurs du lambda Box-Cox
boxcoxv<-BoxCox.ar(ttaux, plotit=F)$mle
boxcoxic<-BoxCox.ar(ttaux, plotit=F)$ci
```

Étant donné que les données sont positives, il est possible d'utiliser la transformée de Box-Cox afin de stabiliser notre processus. On rappelle que la famille des fonctions de puissance est définie de la façon suivante:

$$
\begin{aligned}
g(x) = \frac{x^{\lambda}-1}{\lambda} \times 1_{\{\lambda \neq 0 \}} + \text{ln}(x) \times 1_{\{\lambda = 0 \}}
\end{aligned}
$$

$\lambda$ est donc déterminé en maximisant la fonction de log-vraisemblance de nos données fournie à la \textit{Figure 3}.


On constate que $\lambda = 0.1$ semble être l'estimé MLE situé au centre de l'intervalle de confiance 95%, soit $]-0.5, 0.7[$. Puisque $\lambda = 0$ est dans notre intervale de confiance, cette valeur du paramètre est également à considérer. Dans le cas ou le processus serait à différencier une seule fois, la transformation logarithmique est particuliairement attirante puisqu'elle permet de modéliser non pas le «prix» du taux de change, mais sont rendement comme il est usage de la faire dans le cas d'outils financiers. En effet, soit $Y_t$ le prix d'un outil financier au temps $t$. Le modèle logarithmique modélise le rendement de la façon suivante.[^n1] SI ON GARDE PLUS LOIN LE MODELE ARIMA(0,1,1) CELA REVIENS AU MODELE LOGNORMALE (A MENTIONNER)

[^n1]: WEISHAUS, Abraham, SOA Exam MFE Models for Financial Economics 10th Edition.

$$
Y_t=Y_{t-1}e^{X}
$$
Où \textit{X}, le rendement continu mensuelle, est la variable aléatoire à modéliser. Ainsi, suite à une première différentiation de notre modèle logarithmique, on obtient direction ce rendement. Soit,
$$
log\left(\frac{Y_t}{Y_{t-1}}\right)=X
$$
On conservera alors cette transformation à condition que la première différentiation de notre processus soit stationnaire. Le graphique de cette transformation est affiché à la \textit{Figure 4}.
\newline\newline
On effectue alors le test augmenter de 












Cependant, en analysant le graphique de la fonction d'autocorrélation du logarithme de  notre série, on remarque qu'il n'y a pas de variation notable. Le modèle est toujours non stationnaire. PLUGGER LA THéORIE DE YAN SUR LE MFE PI LES SHIT DE RENDEMENT.

## d)
En revanche, si on effectue une première différentiation du log de notre série, on obtient la série de la figure 4 qui semble plus stable.

```{r diff_log, eval=T, echo=F, fig.height=3.3}
plot(diff(log(ttaux)), type='o', ylab='différencitation du logarithme', main = "Figure 4: différencitation du logarithme")
```

On vérifie alors si une première différentiation amène la stationnarité à notre processus en utilisant le test augmenté de Dickey-Fuller (ADF). En ce concentrant sur la partie autorégressive, on obtient un processus AR(1) associé à notre modèle de la première différenciation du logarithme. Par la suite, on applique le test ADF à 95% et on obtient comme estimé $a/\sigma_a$, $t_{DF} = -9.5181$, avec une p-value de 0.01. Le test semble corroborer que la série chronologique est stationnaire, i.e qu'une première différenciation doit s'appliquer.

## e)
On cherche maintenant le modèle de notre série stationnaire. En observant le graphique ACF et PACF de la figure 5 et 6. Il est à noter que le lag est exprimé en année, ce qui veut dire que 0.1 équivaut à 1/10 d'année et que 0.5 équivaut à 6 mois.

```{r, eval=T, echo=F, fig.height=3.3}
acf(diff(log(ttaux)), main="Figure 5: ACF de la différence du logarithme")
pacf(diff(log(ttaux)), main="Figure 6: PACF de la différence du logarithme")
```

Il semblerait que l'on soit en présence d'un ARIMA(1,1,0) en se fiant au tableau ACF. Pour ce qui est du graphique PACF, on observe une ARIMA(0,1,1). Nous confirmons le tout avec la fonction d'autocorrélation étendue (EACF) ci-dessous.

```{r, eval=T, echo=F, fig.height=3.3}
eacf(diff(log(ttaux)))
```

Ce graphique porte à croire que l'on est en présence d'un modèle ARIMA(0,1,1), i.e. IMA(1). On continue notre analyse en appliquant le critère d'information Bayesienne (BIC).

```{r, eval=T, echo=F, fig.height=3.3}
plot(armasubsets(y=diff(log(ttaux)), nar=3, nma=3, y.name='test',ar.method='ols'))
```

Nous concluons à partir de ce graphique que le meilleur modèle à considérer est l'ARIMA(0,1,3).

Pour résumé, après avoir analyser les différents graphiques de corrélation et les différents tests, nous considérerons les 3 modèles suivants basés sur le logarithme:

* ARIMA(1,1,0);
* ARIMA(0,1,1);
* ARIMA(0,1,3).

## f)
Par la suite, il faut estimer les paramètres de chacun des modèles énumérés en (e) par la méthode du maximum de vraisemblance (MLE).

```{r MLE_ARIMA011, eval=T, echo=F}
ttaux011 <- arima(log(ttaux), order=c(0,1,1), method='ML')
```

Pour l'ARIMA(0,1,1), le paramètre $\theta_1$ de notre MA(1) est de 0.3118, ce qui nous donne le modèle suivant:
$$
log Y_t = log Y_{t-1} + e_t + 0.3118e_{t-1}
$$
```{r MLE_ARIMA110, eval=T, echo=F}
ttaux110 <- arima(log(ttaux), order=c(1,1,0), method='ML')
```

Pour l'ARIMA(1,1,0), le paramètre $\phi_1$ de notre AR(1) est de 0.2933, ce qui nous donne le modèle suivante:

$$
log Y_t = 1,2933\ log Y_{t-1} - 0.2933\ logY_{t-2} + e_t
$$
Pour l'ARIMA(0,1,3), le paramètre $\phi_1$ de notre AR(1) est de 0.2933, ce qui nous donne le modèle suivante:
# voir chapitre 5 pour écrire le modèle

\newpage

# Annexe
## Tableaux et figures
```{r fig1.1, echo=F, fig.height=3.3}
# graphique de la série
plot(ttaux,ylab='USD/Euro', xlab="Temps", type="o", cex=0.5)
abline(h=1)
```
\textbf{Figure 1.} Taux de change US/Euro mensuelle de janvier 1999 à décembre 2015.

```{r fig1.2, eval=TRUE, echo=FALSE,fig.height=3.5}
acf(ttaux, main='')
```
\textbf{Figure 2.} Fonction d'autocorrélation échantillonales du taux de change US/Euro. Le lag étant exprimé en années.

```{r fig1.3, eval=T, echo=F, fig.height=3.3, message=FALSE, warning=FALSE}
BoxCox.ar(ttaux)
```
\textbf{Figure 3.} Fonction du logarithme de vraissemblance de la transformée de Box-Cox de la série chronologique du taux de change US/Euro.

```{r fig1.4, eval=T, echo=F, fig.height=3.3}
plot(log(ttaux), type='o', ylab='log', cex=0.5)
abline(h=0)
```
\textbf{Figure 4.} Logarithme du taux de change US/Euro mensuelle de janvier 1999 à décembre 2015.

\newpage

## Code informatique

```{r ref1.1, ref.label="no1.1", echo=TRUE, eval=FALSE}

```
```{r ADF, eval=F, echo=T}
# Test augmenté de Dickey-Fuller (ADF)
ar(log(ttaux))
adf.test(log(ttaux), k=2)
```
```{r ref2, ref.label="MLE_ARIMA011", eval=F, echo=T}

```

