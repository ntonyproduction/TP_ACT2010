---
title: ''
lang: fr
output:
  pdf_document:
    number_sections: no
    toc: yes
  html_document:
    toc: yes
documentclass: article
---

\newpage

#Introduction au rapport

Le présent rapport se veut être l'analyse de différentes séries chronologiques. Plus précisément, il a pour but de trouver le modèle qui s'ajuste le plus exactement à nos échantillons de données. En bref, le travail consiste à séparer nos bases de données en un échantillon \textit{entraînement} qui sert à trouver un modèle et en un échantillon \textit{test} qui sert à valider la précision du modèle retenu. Le travail est fait à partir de deux bases de données pour un total de cinq variables à modéliser. La première base de données, traitée au numéro 1, contient les données mensuelles du taux de change du dollar américain par rapport à l'euro de janvier 1999 à décembre 2016. La seconde base de données contient une série de statistiques de la \textit{SAAQ}, soit le nombre d'accidents automobiles avec dommages corporels, le nombre de personnes accidentées, le nombre de demandes d'indemnités et le coût total de l'indemnisation (en dollar constant 2015) pour les années 1978 à 2015 inclusivement. Ces données seront traîtées au numéro 2. Afin d'alléger la lecture du présent rapport, le code R est placé en annexe.

\newpage

#Taux de change US/Euro
## Analyse primaire du jeu de données

```{r no1.1, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
library('TSA')
library('tseries')
library('forecast')
library('plotrix')

# Importation des données
#taux <- read.csv2("C:/Users/angag426/Documents/GitHub/TP_ACT2010//Taux_de_change_US_Euro.csv")
taux <- read.csv2("C:/Users/Anthony/Documents/GitHub/TP_ACT2010/Taux_de_change_US_Euro.csv")
#taux <- read.csv2("C:/Users/TEMP/Documents/GitHub/TP_ACT2010/Taux_de_change_US_Euro.csv")
rendement<-taux$US.Euro
anne.mois<-taux$Année.mois

# création de la série chronologique
ttaux<-ts(rendement,start=c(1999,1),end=c(2016,12),frequency = 12)
```

Tout d'abord, nous observons, à la \textit{Figure 15}, la série chronologique du taux de change américain/européen depuis janvier 1999 jusqu'à décembre 2016. Les données ont été collectées de façon quotidienne pour ensuite être transformées en taux mensuelles.

```{r fig1.1, echo=F, fig.height=3}
# graphique de la série
plot(ttaux,ylab='USD/Euro', xlab="Année", type="o", cex=0.5)
abline(h=1)
```
\centerline{\textbf{Figure 1.} Taux de change US/Euro mensuelle de janvier 1999 à décembre 2015.}

\vspace{5mm}

Grâce à une première analyse, on remarque sur le tableau de la fonction d'autocorrélation échantillonale qu'il y a une forte autocorrélation dans notre jeu de données et que celle-ci diminue lentement plus le lag augmente (voir \textit{Figure 2}). On soupçonne donc fortement ce processus d'être non-stationnaire. Il n'est pas nécessaire de tracer le graphique de la fonction d'autocorrélation partielle puisqu'on doit d'abord stationnariser notre processus avant de réellement débuter notre analyse.


```{r fig1.2, eval=TRUE, echo=FALSE,fig.height=3}
acf(ttaux, main='')
```
\centerline{\textbf{Figure 2.} Fonction d'autocorrélation échantillonales du taux de change US/Euro  (lag étant exprimé en années).}

##Test de stationnarité

```{r no1.2, echo=F}
#test augmente de Dickey-Fuller
#ar(ttaux)
ttaux.adf <- adf.test(ttaux, k=ar(ttaux)$order)
```
Le test de stationnarité de Dickey-Fuller est alors effectué sur notre base de données. On remarque que, pour un ordre $k=2$ de processus auto-autoregressif tel que proposé par la fonction \textit{ar} de \textit{R}, notre processus est non stationnaire à un niveau de confiance de $5$% avec une \textit{p-value} très forte de `r round(ttaux.adf$p.value,4)*100`%. On vérifie alors si une transformée Box-Cox est apppropriée à notre modèle avant de tester la stationnarité d'une première différenciation. 

##Transformation des données
```{r no1.3, echo=F,eval=F, warning=F}
#valeurs du lambda Box-Cox
boxcoxv <- BoxCox.ar(ttaux)$mle
boxcoxic <- BoxCox.ar(ttaux)$ci
```

Étant donné que les données sont positives, il est possible d'utiliser la transformée de Box-Cox afin de stabiliser notre processus. On rappelle que la famille des fonctions de puissance est définie de la façon suivante:

$$
\begin{aligned}
g(x) = \frac{x^{\lambda}-1}{\lambda} \times 1_{\{\lambda \neq 0 \}} + \text{ln}(x) \times 1_{\{\lambda = 0 \}}
\end{aligned}
$$

$\lambda$ est donc déterminé en maximisant la fonction de log-vraisemblance de nos données fournie à la \textit{Figure 3}.

```{r fig1.3, eval=T, echo=F, fig.height=3, message=FALSE, warning=FALSE}
BoxCox.ar(ttaux)
```
\textbf{Figure 3.} Fonction du logarithme de vraissemblance de la transformée de Box-Cox de la série chronologique du taux de change US/Euro.

\vspace{5mm}

On constate que $\lambda = 0.1$ semble être l'estimé MLE situé au centre de l'intervalle de confiance 95%, soit $]-0.5, 0.7[$. Puisque $\lambda = 0$ est dans notre intervale de confiance, cette valeur du paramètre est également à considérer. Dans le cas où le processus serait à différencier une seule fois, la transformation logarithmique est particuliairement intéressante puisqu'elle permet de modéliser non pas le «prix» du taux de change, mais sont rendement comme il est usage de le faire dans le cas d'outils financiers. En effet, soit $Y_t$ le prix d'un outil financier au temps $t$, le modèle logarithmique modélise le rendement de la façon suivante[^n1]

[^n1]: WEISHAUS, Abraham, SOA Exam MFE Models for Financial Economics 10th Edition.

$$
Y_t=Y_{t-1}e^{X}
$$
où \textit{X}, le rendement continu mensuel, est la variable aléatoire à modéliser. Ainsi, suite à une première différenciation de notre modèle logarithmique, on obtient direction ce rendement. Soit,
$$
log\left(\frac{Y_t}{Y_{t-1}}\right)=X
$$
On conserve alors cette transformation à condition que la première différenciation de notre processus soit stationnaire. Le graphique de cette transformation est affiché à la \textit{Figure 4}.

```{r fig1.4, eval=T, echo=F, fig.height=3}
plot(log(ttaux), type='o',xlab="Année", ylab='Valeur du logarithme', cex=0.5)
abline(h=0)
```
\centerline{\textbf{Figure 4.} Logarithme du taux de change US/Euro mensuelle de janvier 1999 à décembre 2016.}

\vspace{5mm}

```{r no1.4, echo=F, warning=F}
#ar(diff(log(ttaux)))
ttaux.log.adf <- adf.test(diff(log(ttaux)), k=ar(diff(log(ttaux)))$order)
```

Suite à une première différenciation, on effectue à nouveau le test augmenté de Dickey-Fuller. Avec un processus autorégressif d'ordre 1 tel que suggéré par la fonction $ar$, on remarque cette fois qu'on ne peut rejetter pas l'hypothèse de stationnarité avec un p-value inférieur à `r round(ttaux.log.adf$p.value,4)*100`%. Ainsi, tel que mentionné précédement, la transformation logarithmique est conservée. Le graphique de la première différenciation du logarithme de la série chronologique à l'étude se trouve ci-dessous à la \textit{Figure 5}.

```{r fig1.5, eval=T, echo=F, fig.height=3}
plot(diff(log(ttaux)), type='o',xlab="Année", ylab='différenciation du logarithme', cex=0.5)
abline(h=0)
```
\centerline{\textbf{Figure 5.} Première différenciation du logarithme du taux de change US/Euro mensuelle de janvier 1999 à décembre 2015.}

## Identification du modèle

Maintenant que la série est stationnaire, on peut en identifier le modèle. La première différenciation du logarithme du taux de change US/Euro est dorénavent notre modèle de base. On fera ainsi référence à ce modèle par défaut. 
\newline\newline
On observe les fonctions d'autocorrélation et d'autocorrélation partielle affichées aux \textit{Figure 6} et \textit{Figure 7} de cette série. On remarque que la fonction d'autocorrélation suggère fortement un modèle à moyenne mobile d'ordre 1, soit IMA(1,1) pour notre modèle tansformé alors que la fonction d'autocorrélation partielle suggère un modèle autorégressif d'ordre 1, soit ARI(1,1). On testera alors également le modèle ARIMA(1,1,1) qui est suggéré par la combinaison de ces graphiques.

```{r fig1.6, eval=T, echo=F, fig.height=3}
acf(diff(log(ttaux)), main='')
```
\textbf{Figure 6.} Fonction d'autocorrélation de la première différenciation du logarithme du taux de change US/Euro mensuelle de janvier 1999 à décembre 2015.

```{r fig1.7, eval=T, echo=F, fig.height=3}
pacf(diff(log(ttaux)), main='', ylab='PACF')
```
\textbf{Figure 7.} Fonction d'autocorrélation partielle de la première différenciation du logarithme du taux de change US/Euro mensuelle de janvier 1999 à décembre 2015.

\hspace{5mm}

On observe ensuite la fonction d'autocorrélation étendue afin de voir si ce test à un autre modèle à proposer. Le tableau de la fonction EACF est à la \textit{Table I}. On observe d'abord de ce tableau que le modèle IMA(1,1) est suggéré pour la différenciation de notre modèle transformé initial. On peut également chercher à savoir si la valeur du $o$ en ARMA(0,1) et du $x$ en ARMA(1,1) sont significativement différent de leur valeurs inverses ($x$ pour le $o$ et vice versa), sans quoi le modèle ARIMA(1,1,1) serait également suggeré par le tableau EACF. Cependant, comme il fut déjà décidé d'évaluer la pertinance du modèle ARIMA(1,1,1) suite à l'analyse des fonction d'autocorrélation et d'autocorrélation partielle, il n'est pas nécessaire de tester la pertinance de ce modèle selon le EACF.

\vspace{5mm}

```{r table1.1, eval=T, echo=F, fig.height=3}
eacf(diff(log(ttaux)))
```
\textbf{Table I.} Tableau de la fonction EACF de la première différenciation du logarithme du taux de change US/Euro mensuelle de janvier 1999 à décembre 2015.

\vspace{5mm}

Le dernier test effectué pour ajuster un modèle à notre série chronologique est le test du BIC. Le tableau du BIC (tracé à la \textit{Table II}) propose un modèle ARI(1,1). L'ordre maximal de moyenne mobile pour ce test a été établi à 1, sans quoi la fonction \textit{armasubsets} permettant de tracer le tableau du BIC retourne des avertissements de forte dépendance linéaire entre les paramètres. 

```{r table1.2, eval=T, echo=F, fig.height=3}
plot(armasubsets(y=diff(log(ttaux)), nar=10, nma=1, y.name='test',ar.method='ols'))
```
\textbf{Table II.} Tableau de la fonction BIC de la première différenciation du logarithme du taux de change US/Euro mensuelle de janvier 1999 à décembre 2015.

\newpage

On étudiera ainsi les quatre modèles proposés par les tests précédents qui peuvent être résumé de la façon suivante;

$$
\begin{tabular}{| l | r |}
  \hline
  \multicolumn{2}{|c|}{Résumé}\\
  \hline
  Tests et tableaux & Modèles suggérés \\ \hline \hline
  ACF & IMA(1,1) \\
    & ARIMA(1,1,1) \\ \hline
  PACF & ARI(1,1) \\
    & ARIMA(1,1,1)\\ \hline
  EACF & IMA(1,1) \\ \hline
  BIC & ARI(1,1) \\ 
  \hline
 \end{tabular}
$$

On teste alors respectivement les modèles ARI(1,1), IMA(1,1) et ARIMA(1,1,1) suivants

$$
\begin{aligned}
W_t-\mu &= \phi (W_{t-1}-\mu) + e_t \qquad \qquad \qquad &(1)\\
W_t -\mu&= e_t-\theta e_{t-1} \qquad \qquad \qquad &(2)\\
W_t- \mu&= \phi (W_{t-1}-\mu) + e_t-\theta e_{t-1} \qquad \qquad \qquad &(3)\\
\end{aligned}
$$
Où
$$
W_t=\nabla \text{ln}(Y_t)
$$
et $e_t$ est un bruit blanc et $\mu$ la moyenne de la série chronologique non-centrée. Il est ici important de noter que les paramètres estimés $\hat{\theta}$ et $\hat{\phi}$ ne seront pas les mêmes dans les trois modèles et devront donc être estimés pour chacun d'entre eux.

##Estimation des paramètres des différents modèles et tests de résidus

```{r no1.5, echo=F, warning=F}
#IMA(1,1)
ttaux011 <- arima(log(ttaux), order=c(0,1,1), method='ML')
#ARI(1,1)
ttaux110 <- arima(log(ttaux), order=c(1,1,0), method='ML')
#ARIMA(1,1,1)
ttaux111 <- arima(log(ttaux), order=c(1,1,1), method='ML')
#moyenne
ttaux.log.mu <- mean(diff(log(ttaux)))

#test des residus (shapiro-wilk)
ttaux011.sw <- shapiro.test(rstandard(ttaux011))
ttaux110.sw <- shapiro.test(rstandard(ttaux110))
ttaux111.sw <- shapiro.test(rstandard(ttaux111))
#test de dependance des residus (run.test)
ttaux011.rt <- runs(residuals(ttaux011))$pvalue
ttaux110.rt <- runs(residuals(ttaux110))$pvalue
ttaux111.rt <- runs(residuals(ttaux111))$pvalue
#test de Ljung-Box
ttaux011.lb <- Box.test(residuals(ttaux011), lag= 10, type='Ljung-Box', fitdf=1)
ttaux011.lb <- Box.test(residuals(ttaux110), lag= 10, type='Ljung-Box', fitdf=1)
ttaux111.lb <- Box.test(residuals(ttaux111), lag= 10, type='Ljung-Box', fitdf=1)
#overfiting
ttaux012<-arima(log(ttaux), order=c(0,1,2), method='ML')
```
On estime alors les valeurs des différents paramètres selon la méthode du maximum de vraisemblance à l'aide de la fonction $arima$. Les résultats se trouvent à la \textit{Table III}.

$$
\begin{tabular}{| l c |c|c| r |}
  \hline
  Modèles & Paramètres & Valeurs estimés & AIC & Logarithme du max \\
  & & & &  de vraisemblance \\ \hline \hline
  IMA(1,1) & $\mu$ & 0 & $`r round(ttaux011$aic,0)`$ & $`r round(ttaux011$loglik,0)`$\\
    & $\theta$ & $`r round(ttaux011$coef,4)`$&  &\\ \hline
  IMA(1,2) & $\mu$ & 0 & $`r round(ttaux012$aic,0)`$ & $`r round(ttaux012$loglik,0)`$\\
    & $\theta_1$ & $`r round(ttaux012$coef[1],4)`$&  &\\
    & $\theta_2$ & $`r round(ttaux012$coef[2],4)`$&  &\\ \hline
  ARI(1,1) & $\mu$ & 0& $`r round(ttaux110$aic,0)`$ &$`r round(ttaux110$loglik,0)`$\\
    & $\phi$ & $`r round(ttaux110$coef,4)`$&  &\\ \hline
  ARIMA(1,1,1) & $\mu$ & 0& $`r round(ttaux111$aic,0)`$ &$`r round(ttaux111$loglik,0)`$\\
    & $\theta$ & `r round(ttaux111$coef[2],4)`&  &\\
    & $\phi$ & `r round(ttaux111$coef[1],4)`&  &\\
  \hline
 \end{tabular}
$$
\textbf{Table III.} Tableau des valeurs estimées des différents coefficients obtenus de la fonction \textit{arima} pour les trois modèles à l'étude du logarithme du taux de change US/Euro.

\vspace{5mm}

On peut alors faire l'analyse de nos résidus pour faire notre choix parmi nos trois modèles. Les résidus standardisés de ces trois modèles sont tracés à la \textit{Figure 8}. On remarque d'abord que ces trois courbes se superposent et ne peuvent donc pas être utilisées pour sélectionner un modèle. De plus, on remarque de ce graphique que les valeurs de résidus supérieures à 2 en valeur absolue sont relativement fréquentes ce qui nous poussent à questionner la théorie selon laquelle les résidus suivent une loi normale. En effet, dans le cas des résidus normaux, les valeurs supérieurs à 2 en valeur absolue ne devrait apparaître qu'environ `r round(1-pnorm(2),4)*100`% du temps. La fréquence observée est nettement supérieur.

```{r fig1.8, eval=T, echo=F, fig.height=4, fig.width=9}
plot(rstandard(ttaux011), ylab='résidus standardisés', type='l', col='green')
lines(rstandard(ttaux110), col='red')
lines(rstandard(ttaux111), col='blue')
abline(h=0)
```
\textbf{Figure 8.} Superposition des graphiques des résidus standardisés des modèles ARI(1,1), IMA(1,1) et ARIMA(1,1,1) du logarithme du taux de change US/Euro.

\vspace{5mm}

On regarde ensuite les histogrammes et les graphiques QQ aux \textit{Table IV} et \textit{Figure 9} des résidus. On remarque que les trois histogrammes semblent tracer avec efficacité la cloche de la loi normal. De plus, on remarque des trois graphiques QQ, que les résidus observés sont en tout point simiaire à leur valeur théorique en cas de normalité. Ces deux test semblent donc justifier la distribution normale des résidus de nos trois modèles.

```{r table1.4, eval=T, echo=F, fig.height=3, fig.width=2.3}
hist(rstandard(ttaux011), xlab='',ylab='fréquence', main='IMA(1,1)')
hist(rstandard(ttaux110), xlab='résidus standardisés',ylab='', main='ARI(1,1)')
hist(rstandard(ttaux111), xlab='',ylab='', main='ARIMA(1,1,1)')
```
\textbf{Table IV.} Histogramme des résidus standardisés des modèles étuidés du logarithme du taux de change US/Euro.

```{r fig1.9, eval=T, echo=F, fig.height=3, fig.width=2.3}
qqnorm(rstandard(ttaux011), main='IMA(1,1)',ylab='Quantiles observés',xlab='',cex=0.2)
qqnorm(rstandard(ttaux110),main='ARI(1,1)',xlab='Quantiles théoriques', ylab='',cex=0.2)
qqnorm(rstandard(ttaux111),main='ARIMA(1,1,1)',xlab='', ylab='',cex=0.2)
```
\textbf{Figure 9.} Graphiques QQ des modèles étudiés du logarithme du taux de change US/Euro.

\vspace{5mm}

On effectue alors, pour tenter de départager nos modèles, le test de Shapiro-Wilk évaluant le dégré de corrélation entre les quantiles des résidus standardisés et la loi normale standard. L'hypothèse nulle étant que la distribution des résidus suit une loi normale On confirme, avec des p-values respectives de `r round(ttaux011.sw$p.value,4)`, `r round(ttaux110.sw$p.value,4)` et `r round(ttaux111.sw$p.value,4)`, que les résidus engendrés par les modèles IMA(1,1), ARI(1,1) et ARIMA(1,1,1) sont tous normalement distribués pour un test à un niveau de confiance $\alpha$ de 5%.
\newline\newline
On cherche également un modèle dont les résidus sont indépendants, sans quoi on ne retrouve pas un bruit blanc. On effectue alors le run-test sur nos modèles pour tester l'indépendance entre les résidus. Comme l'hypothèse nulle est que les résidus sont indépendants, on conclut à un niveau de confiane de $\alpha=5$% qu'on ne peut rejetter l'hypothèse nulle si la p-value est supérieur à 5%. On trouve alors, pour les modèles IMA(1,1), ARI(1,1) et ARIMA(1,1,1) des valeurs respectives de `r round(ttaux011.rt,4)`, `r round(ttaux110.rt,4)` et `r round(ttaux111.rt,4)` pour les p-value de leur run-tests. Ainsi, aucun de ces modèles rejettent l'hypothèse de normalité.
\newline\newline
On effectue un dernier test sur nos modèles afin de les départager. Le test de Ljung-Box à pour hypothèse nul que le modèle testé sur notre série chronologique est approprié. On remarque des \textit{Figure 10} à \textit{Figure 12} que les p-values des trois modèle ne permettent pas de rejetter l'hypothèse nulle. Les trois modèles sont donc, selon ce test, appropriés pour modéliser la série chronologique à l'étude.

```{r fig1.10, eval=T, echo=F, fig.height=4.1, warning=F}
tsdiag(ttaux011, gof=15, omit.inital=F)
```
\textbf{Figure 10.} Graphiques des résidus standardisés, de l'autocorrélation des résidus et des p-values du test de Ljung-Box du modèle IMA(1,1) pour le logarithme du taux de change US/Euro.

```{r fig1.11, eval=T, echo=F, fig.height=4.1, warning=F}
tsdiag(ttaux110, gof=15, omit.inital=F)
```
\textbf{Figure 11.} Graphiques des résidus standardisés, de l'autocorrélation des résidus et des p-values du test de Ljung-Box du modèle ARI(1,1) pour le logarithme du taux de change US/Euro.

```{r fig1.12, eval=T, echo=F, fig.height=4.1, warning=F}
tsdiag(ttaux111, gof=15, omit.inital=F)
```
\textbf{Figure 12.} Graphiques des résidus standardisés, de l'autocorrélation des résidus et des p-values du test de Ljung-Box du modèle ARIMA(1,1,1) pour le logarithme du taux de change US/Euro.

\vspace{5mm}

On se trouve ainsi dans l'impossibilité de départager nos modèles à l'aide de l'étude de leurs résidus en plus de voir tous ces modèles être jugés appropriés selon le test de Ljung-Box. On choisit alors de conserver le modèle IMA(1,1) parce qu'il est simple, parce que la valeur maximale de sa fonction de vraisemblance est la plus élevée et parce que la valeur du test AIC est la plus faible selon la \textit{Table III}. De plus, ce modèle nous permet de retrouver un modèle de loi lognormale pour modéliser le taux US/Euro qui est un modèle fortement utilisé dans le monde de la mathématique financière.
\newline\newline
On regarde, pour finaliser le choix de modèle, la question de l'\textit{overfiting}. Comme le modèle ARIMA(1,1,1) a déjà été testé, il suffit ici d'observer le comportement du modèle IMA(1,2). On remarque de la \textit{Table III} que le paramètre $\theta_1$ du modèle IMA(1,2) est très près du paramètre $\theta$ du modèle IMA(1,1) en plus d'avoir un paramètre $\theta_2$ avoisinant 0. Sachant que $\hat{\theta}_2 \approx$ `r round(arima(x = log(ttaux), order = c(0, 1, 2), method = "ML")$coef['ma2'], 4)` et que son écart-type est de 0.0645, l'intervalle de confiance de $\hat\theta_2$ est $IC = \hat\theta_2 \pm Z_{1-\alpha/2} \sqrt{Var(\hat\theta_2)} =$ [`r round(0.0207-qnorm(1-0.05/2)*0.0645,4)`, `r round(0.0207+qnorm(1-0.05/2)*0.0645, 4)`]. Puisque 0 est dans l'intervalle de confiance, on peut négliger ce paramètre et on peut alors écarter ce modèle. Le modèle IMA(1,1) est donc celui avec lequel les prédictions seront effectuées. 

##Prédictions
```{r no1.6, echo=F}
ttaux011.pred<-predict(ttaux011, n.ahead=10)
ttaux011.inf<-ttaux011.pred$pred-qnorm(0.975)*ttaux011.pred$se
ttaux011.sup<-ttaux011.pred$pred+qnorm(0.975)*ttaux011.pred$se
#modele final
ttt.pred<-round(exp(ttaux011.pred$pred+0.5*ttaux011.pred$se^2),4)
ttt.inf<-round(exp(ttaux011.inf), 4)
ttt.sup<-round(exp(ttaux011.sup), 4)
err<-rendement[217:226] - ttt.pred
```

On peut alors effectuer les prédictions des taux de changes US/Euro des mois de l'année 2017 grâce au modèle suivant

$$
\nabla \text{ln} (Y_t)=e_t-`r round(ttaux011$coef,4)` e_{t-1}
$$
Il est important de noter que, puisqu'une transformation logarithmique a été effectuée sur les données du taux de change US/Euro, le prédicteur de $Y_t$, $\hat{Y}_t(l)$, sera tel que

$$
\hat{Y}_t(l)=\text{exp}\left(\hat{Z}_t(l)+\frac{\text{Var}[e_t(l)]}{2}\right)
$$
Où $\hat{Z}_t(l)$ est le prédicteur de $Z_t$ défini tel que $Z_t=\text{ln}Y_t$.
\newline\newline
Le graphique des valeurs prédites se trouvent aux graphiques \textit{Figure 13} et \textit{Figure 14}. On remarque d'abord du second graphique que toutes les valeurs du taux de change US/Euro observées en 2017 se trouvent à l'intérieur de l'intervalle de confiance de nos prédictions. L'intervalle de confiance est fait à un niveau de confiance de 95%.

```{r fig1.13, eval=T, echo=F,fig.height=3.2, fig.width=7.5, warning=F}
plot(ttaux011, n.ahead=12, ylab='Valeur du logarithme', xlab='Années', cex=0.5)
abline(h=0)
```
\textbf{Figure 13.} Logarithme du taux de change US/Euro mensuelle de janvier 1999 à décembre 2016 avec prédictions pour l'année 2017 et intervalle de confiance à 95%.

```{r fig1.14, eval=T, echo=F,fig.height=3.7, warning=F}
plotCI(ttt.pred, ylab='Taux de change US/Euro', xlab='Mois', ylim=c(0.8, 1.3), li=ttt.inf, ui=ttt.sup)
lines(rendement[217:226], type='o', col='Red')
legend(x=0.8,y=0.92,legend=c("Prédictions","Observations"),
       lty=c(1,1),lwd=c(2,2),col=c("black","red"))
```
\textbf{Figure 14.} Comparaison entre les valeurs prédites du taux de change US/Euro grâce au modèle IMA(1,1) avec intervalle de confiance et les valeurs observerées pour l'année 2017.

```{r no1.7, echo=F}
a <- ts(rendement,start=c(1999,1),end=c(2017,10),frequency = 12)[217:226] #valeurs des 10 derniers mois notre série
```


On retrouve également les valeurs des prédictions du taux de change US/Euro pour les 10 premiers mois de l'année 2017 dans le tableau suivant

| Mois  | $Y_{t+l}$  | $\hat Y_t(l)$  | $\hat e_t(l)$  | $\sqrt{Var(\hat Y_t(l))}$  | Borne inf. de prédiction  | Borne sup. de prédiction  |
|:------:|:---:|:---:|:---:|:-----------:|:------:|:------:|
| Janvier    |`r round(a[1], 4)` |`r round(exp(ttaux011.pred$pred[1] + 0.5*ttaux011.pred$se[1]^2), 4)`   | `r round(exp(ttaux011.pred$pred[1] + 0.5*ttaux011.pred$se[1]^2) - a[1], 4)*(-1)` | `r round(ttaux011.pred$se[1], 4)`   | `r ttt.inf[1]`   | `r ttt.sup[1]` |   
| Février    |`r round(a[2], 4)` |`r round(exp(ttaux011.pred$pred[2] + 0.5*ttaux011.pred$se[2]^2), 4)`   | `r round(exp(ttaux011.pred$pred[2] + 0.5*ttaux011.pred$se[2]^2) - a[2], 4)*(-1)` | `r round(ttaux011.pred$se[2], 4)`   | `r ttt.inf[2]`  | `r ttt.sup[2]`  |   
| Mars       |`r round(a[3], 4)` |`r round(exp(ttaux011.pred$pred[3] + 0.5*ttaux011.pred$se[3]^2), 4)`   | `r round(exp(ttaux011.pred$pred[3] + 0.5*ttaux011.pred$se[3]^2) - a[3], 4)*(-1)` | `r round(ttaux011.pred$se[3], 4)`   | `r ttt.inf[3]`  |  `r ttt.sup[3]` |   
| Avril      |`r round(a[4], 4)` |`r round(exp(ttaux011.pred$pred[4] + 0.5*ttaux011.pred$se[4]^2), 4)`   | `r round(exp(ttaux011.pred$pred[4] + 0.5*ttaux011.pred$se[4]^2) - a[4], 4)*(-1)` | `r round(ttaux011.pred$se[4], 4)`   | `r ttt.inf[4]`  |  `r ttt.sup[4]` |   
| Mai        |`r round(a[5], 4)` |`r round(exp(ttaux011.pred$pred[5] + 0.5*ttaux011.pred$se[5]^2), 4)`   | `r round(exp(ttaux011.pred$pred[5] + 0.5*ttaux011.pred$se[5]^2) - a[5], 4)*(-1)` | `r round(ttaux011.pred$se[5], 4)`   | `r ttt.inf[5]`  |  `r ttt.sup[5]` |   
| Juin       |`r round(a[6], 4)` |`r round(exp(ttaux011.pred$pred[6] + 0.5*ttaux011.pred$se[6]^2), 4)`   | `r round(exp(ttaux011.pred$pred[6] + 0.5*ttaux011.pred$se[6]^2) - a[6], 4)*(-1)` | `r round(ttaux011.pred$se[6], 4)`   | `r ttt.inf[6]`  |  `r ttt.sup[6]` |   
| Juillet    |`r round(a[7], 4)` |`r round(exp(ttaux011.pred$pred[7] + 0.5*ttaux011.pred$se[7]^2), 4)`   | `r round(exp(ttaux011.pred$pred[7] + 0.5*ttaux011.pred$se[7]^2) - a[7], 4)*(-1)` | `r round(ttaux011.pred$se[7], 4)`   | `r ttt.inf[7]`  |  `r ttt.sup[7]` |   
| Août       |`r round(a[8], 4)` |`r round(exp(ttaux011.pred$pred[8] + 0.5*ttaux011.pred$se[8]^2), 4)`   | `r round(exp(ttaux011.pred$pred[8] + 0.5*ttaux011.pred$se[8]^2) - a[8], 4)*(-1)` | `r round(ttaux011.pred$se[8], 4)`   | `r ttt.inf[8]`  |  `r ttt.sup[8]` |   
| Septembre  |`r round(a[9], 4)` |`r round(exp(ttaux011.pred$pred[9] + 0.5*ttaux011.pred$se[9]^2), 4)`   | `r round(exp(ttaux011.pred$pred[9] + 0.5*ttaux011.pred$se[9]^2) - a[9], 4)*(-1)` | `r round(ttaux011.pred$se[9], 4)`   | `r ttt.inf[9]`  |  `r ttt.sup[9]` |   
| Octobre    |`r round(a[10], 4)` |`r round(exp(ttaux011.pred$pred[10] + 0.5*ttaux011.pred$se[10]^2), 4)`  | `r round(exp(ttaux011.pred$pred[10] + 0.5*ttaux011.pred$se[10]^2) - a[10], 4)*(-1)` | `r round(ttaux011.pred$se[10], 4)`  | `r ttt.inf[10]`  |  `r ttt.sup[10]` |

où $\hat e_t(l) = Y_{t+l} - \hat Y_{t}(l)$ représente l'erreur de prédiction.

## Conclusion
En observant le graphique et la valeur de nos prédictions, on observe que celles-ci semblent converger vers une valeur bien précise. Cependant, plus on tente de prédire pour des périodes éloignées, plus notre intervalle de confiance grandit et plus notre incertitude devient élevée.

\newpage

#SAAQ
## Analyse primaire du jeu de données
```{r no2.1, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
# Importation des données
#saaq <- read.csv2("C:/Users/angag426/Documents/GitHub/TP_ACT2010/SAAQ-2015.csv")
saaq <- read.csv2("C:/Users/Anthony/Documents/GitHub/TP_ACT2010/SAAQ-2015.csv")

naadc<-ts(saaq$NAADC,start=c(saaq$Année[1],1),end=c(saaq$Année[length(saaq$Année)],1), frequency=1)
npa<-ts(saaq$NPA,start=saaq$Année[1],end=saaq$Année[length(saaq$Année)])
ndi<-ts(saaq$NDI,start=saaq$Année[1],end=saaq$Année[length(saaq$Année)])
cti<-ts(saaq$CTI,start=saaq$Année[1],end=saaq$Année[length(saaq$Année)])

naadc[tsoutliers(naadc)$index]<-tsoutliers(naadc)$replacements
npa[tsoutliers(npa)$index]<-tsoutliers(npa)$replacements
ndi[tsoutliers(ndi)$index]<-tsoutliers(ndi)$replacements
cti[tsoutliers(cti)$index]<-tsoutliers(cti)$replacements
```

Tout d'abord, on fait l'analyse du jeu de données en lien avec la SAAQ. On étudie les valeurs de nos quatres séries chronologiques depuis 1978 jusqu'en 2015. Cela comprend la série du nombre d'accidents avec dommages corporels (NAADC), du nombre de personnes accidentés (NPA), du nombre de demandes d'indemnités (NDI), et du coût total de l'indemnisation en millions de dollars, et en dollars constants 2015 (CTI), le tout sur une base annuelle. Puisque la banque de données semble comporter certaines valeurs aberrantes, on applique des changements à ces données grâce à la commande *tsoutliers*. Les nouvelles séries chronologiques sont affichées ci-dessous dans la \textit{Figure 15}

```{r fig2.1.1, echo=F, out.width='.49\\linewidth', fig.height=5, fig.align="left", fig.show='hold',fig.align='center'}
# graphiques de la série
plot(naadc,ylab="Fréquence", xlab="Année", main="Nbre d'accidents avec dommages corporels (auto)", type="o", cex=0.5, col="red")
plot(npa,ylab="Fréquence", xlab="Année", main="Nbre de personnes accidentées", type="o", cex=0.5, col="blue")
```

```{r fig2.1.2, echo=F, out.width='.49\\linewidth', fig.height=5, fig.align="left", fig.show='hold',fig.align='center'}
# graphiques de la série
plot(ndi,ylab="Fréquence", xlab="Année", main="Nbre de demandes d'indemnités", type="o", cex=0.5, col="green")
plot(cti,ylab="Coût", xlab="Année", main="Coût total de l'indemnisation", type="o", cex=0.5)
```
\textbf{Figure 15.} Graphiques des différentes séries chronologiques après modification des données aberrantes depuis 1978 jusqu'en 2015 (NAADC en rouge, NPA en bleu, NDI en vert et CTI en noir).

\vspace{5mm}

On remarque prinicipalement deux choses de ces graphiques. D'abord, les trois premières séries chronologiques semblent très fortement corrélées. Ce constat aurait également pu découler de la définition même de ces variables qui sont logiquement, très fortement corrélées.

##Test de stationnarité
```{r no2.2, echo=F}
#test augmente de Dickey-Fuller
naadc.adf <- adf.test(naadc, k=ar(naadc)$order)
npa.adf <- adf.test(npa, k=ar(npa)$order)
ndi.adf <- adf.test(ndi, k=ar(ndi)$order)
cti.adf <- adf.test(cti, k=ar(cti)$order)
```

L'analyse débute encore une fois par les tests de stationnarités. On commence ainsi par effectuer le test de Dickey-Fuller sur notre modèle pour tester ça stationnarité. Le test a été expliqué au numéro précédent. Nous obtenons les résultats suivants

| Série chrono.  | coeff. *k* dans AR(*k*)  | p-value  | Stationnarité  |
|:---:|:---:|:---:|:---:|
| NAADC  | `r ar(naadc)$order`  | `r round(naadc.adf$p.value, 4)`  | Non  |
| NPA  | `r ar(npa)$order`  | `r round(npa.adf$p.value, 4)`  | Non  |
| NDI  | `r ar(ndi)$order`  | `r round(ndi.adf$p.value, 4)`  | Non  |
| CTI  | `r ar(cti)$order`  | `r round(cti.adf$p.value, 4)`  | Non  |

Étant donné que chacune des séries chronologiques a une p-value > 5%, on ne peut pas rejeter l'hypothèse nulle du test qui stipule que la série est non-stationnaire. Une première différenciation s'impose. Cependant, il faudrait d'abord observer si une transformation est approprié pour stabiliser la variance. On effectue ainsi la transformation de Box-Cox.

##Transformation des données

```{r fig2.2.1, echo=F, out.width='.49\\linewidth', fig.height=5, fig.align="left", fig.show='hold',fig.align='center', warning=F}
# graphiques de box cox
boxcoxv_naadc <- BoxCox.ar(naadc, lambda=seq(-3,1,0.01), main="Box-Cox NAADC")$mle
boxcoxic_naadc <- BoxCox.ar(naadc,lambda=seq(-3,1,0.01), main="Box-Cox NAADC")$ci

boxcoxv_npa <- BoxCox.ar(npa, lambda=seq(-3,1,0.01), main="Box-Cox NPA")$mle
boxcoxic_npa <- BoxCox.ar(npa,lambda=seq(-3,1,0.01), main="Box-Cox NPA")$ci
```

```{r fig2.2.2, echo=F, out.width='.49\\linewidth', fig.height=5, fig.align="left", fig.show='hold',fig.align='center', warning=F}
# graphiques de box cox
boxcoxv_ndi <- BoxCox.ar(ndi, lambda=seq(-2,1.5,0.01), main="Box-Cox NDI")$mle
boxcoxic_ndi <- BoxCox.ar(ndi,lambda=seq(-2,1.5,0.01), main="Box-Cox NDI")$ci

boxcoxv_cti <- BoxCox.ar(cti, lambda=seq(-0.5,2.5,0.01), main="Box-Cox CTI")$mle
boxcoxic_cti <- BoxCox.ar(cti,lambda=seq(-0.5,2.5,0.01), main="Box-Cox CTI")$ci
```

On peut donc observer grâce au maximum de vraisemblance que les valeurs de $\lambda$ sont approximativement `r round(boxcoxv_naadc, 0)`, `r round(boxcoxv_npa, 0)`, `r round(boxcoxv_ndi, 0)` et `r round(boxcoxv_cti, 0)` pour les séries NAADC, NPA, NDI et CTI respectivement. On note qu'il n'y a donc pas de transformation a appliquer à la série CTI.

On peut alors, tester la stationnarité de la première différenciation des données transformées encore à l'aide du test de Dikey-Fuller. Ce test renvoie cette fois les valeurs suivantes

| Série chrono.  | coeff. *k* dans AR(*k*)  | p-value  | Stationnarité  |
|:---:|:---:|:---:|:---:|
| NAADC  | `r ar(diff(naadc^-1))$order`  | `r round(adf.test(diff(naadc^-1), k=ar(diff(naadc^-1))$order)$p.value, 4)`  | Oui  |
| NPA  | `r ar(diff(npa^-1))$order`  | `r round(adf.test(diff(npa^-1), k=ar(diff(npa^-1))$order)$p.value, 4)`  | Oui  |
| NDI  | `r ar(diff(log(ndi)))$order`  | `r round(adf.test(diff(log(ndi)), k=ar(diff(log(ndi)))$order)$p.value, 4)`  | Oui  |
| CTI  | `r ar(diff(cti))$order`  | `r round(adf.test(diff(cti), k=ar(diff(cti))$order)$p.value, 4)`  | Oui  |

On remarque cette fois que la p-value de chaque modèle est inférieure à 5%, ce qui nous permet d'affirmer la stationnarité de ceux-ci. On peut maintenant passer à l'étape du choix du modèle.

##Identification du modèle
On affiche les graphiques de nos nouvelles séries chronologiques ci-dessous après avoir apporté la transformation et la différenciation à chacun

```{r fig2.3.1, echo=F, out.width='.49\\linewidth', fig.height=5, fig.align="left", fig.show='hold',fig.align='center', warning=F}
# graphiques des nouvelles séries
plot(diff(naadc^-1), xlab='Temps', ylab='', type='o')
plot(diff(npa^-1), xlab='Temps', ylab='', type='o')
```

```{r fig2.3.2, echo=F, out.width='.49\\linewidth', fig.height=5, fig.align="left", fig.show='hold',fig.align='center', warning=F}
# graphiques des nouvelles séries
plot(diff(log(ndi)), xlab='Temps', ylab='', type='o')
plot(diff(cti), xlab='Temps', ylab='', type='o')
```

On poursuit en analysant les fonctions d'autocorrélation de chaque série transformée tel qui suit

```{r fig2.4.1, echo=F, out.width='.49\\linewidth', fig.height=5, fig.align="left", fig.show='hold',fig.align='center', warning=F}
# graphiques des ACF
acf(diff(naadc^-1), ylab='ACF')
acf(diff(npa^-1), ylab='ACF')
```

```{r fig2.4.2, echo=F, out.width='.49\\linewidth', fig.height=5, fig.align="left", fig.show='hold',fig.align='center', warning=F}
# graphiques des ACF
acf(diff(log(ndi)), ylab='ACF')
acf(diff(cti), ylab='ACF')
```
**FIGURE WHATEVER ACF**

Les graphiques ci-dessus suggère l'absence de coefficent de moyenne mobile dans notre série. Qu'en est-il de l'autocorrélation partielle?

```{r fig2.5.1, echo=F, out.width='.49\\linewidth', fig.height=5, fig.align="left", fig.show='hold',fig.align='center', warning=F}
# graphiques des PACF
pacf(diff(naadc^-1), ylab='PACF')
pacf(diff(npa^-1), ylab='PACF')
```

```{r fig2.5.2, echo=F, out.width='.49\\linewidth', fig.height=5, fig.align="left", fig.show='hold',fig.align='center', warning=F}
# graphiques des PACF
pacf(diff(log(ndi)), ylab='PACF')
pacf(diff(cti), ylab='PACF')
```
**FIGURE WHATEVER PACF**

Les graphiques de l'autocorrélation partielle semblent écarter la présence d'autorégression dans la différentiation de nos modèles. L'analyse des graphiques de l'autocorrélation et de l'autocorrélation partielle suggère donc un bruit blanc comme modèle à considérer pour chacune des séries.

On effectue un autre test pour ajuster un modèle aux séries chronologiques présentement à l'étude, le test de l'autocorrélation étendu. On obtient les tables qui suivent

```{r fig2.6.1, echo=F, eval=F, out.width=c('500px', '300px'), fig.show='hold', fig.align="left"}
# graphiques des EACF
eacf(diff(naadc^-1), ar.max=5,ma.max=10)
eacf(diff(npa^-1), ar.max=5,ma.max=10)
```

```{r fig2.6.2, echo=F, eval=F, out.width=c('500px', '300px'), fig.show='hold',fig.align="left"}
# graphiques des EACF
eacf(diff(log(ndi)), ar.max=5,ma.max=10)
eacf(diff(cti), ar.max=5,ma.max=10)
```

\includegraphics{eacf_naadc}
\includegraphics{eacf_npa}
\includegraphics{eacf_ndi}
\hspace{7.5mm} \includegraphics{eacf_cti}

**FIGURE WHATEVER EACF**

Les graphiques provenant des tests d'autocorrélation étendue nous proposent de façon respective une ARMA(0,0), ARMA(0,0), ARMA(0,1) et ARMA(0,0) pour les modèles transformés et différenciés NAADC, NPA, NDI et CTI.

Il ne reste plus qu'à effectuer le critère du BIC pour conclure quels modèles seront retenus.

```{r fig2.7.1, echo=F, out.width='.49\\linewidth', fig.height=5, fig.align="left", fig.show='hold',fig.align='center', warning=F}
# graphiques des BIC
plot(armasubsets(y=diff(naadc^-1), nar=3, nma=3, y.name = "test", ar.method = "ols"))
plot(armasubsets(y=diff(npa^-1), nar=3, nma=3, y.name = "test", ar.method = "ols"))
```

```{r fig2.7.2, echo=F, out.width='.49\\linewidth', fig.height=5, fig.align="left", fig.show='hold',fig.align='center', warning=F}
# graphiques des BIC
plot(armasubsets(y=diff(log(ndi)), nar=3, nma=3, y.name = "test", ar.method = "ols"))
plot(armasubsets(y=diff(cti), nar=3, nma=1, y.name = "test", ar.method = "ols"))
```

Les graphiques provenant du critère BIC nous proposent de façon respective une AR(1), MA(1), AR(1) et MA(3) (avec $\theta_1$ et $\theta_2$ égal à 0) pour les modèles transformés et différenciés NAADC, NPA, NDI et CTI. Il est pertinent de mentionner que le BIC tente de renvoyer le modèle ARMA(p,q) qui s'applique le mieux à la série sans jamais proposer le bruit blanc.

Les modèles testés seront donc les suivants:

* NDAAC

$$
\begin{aligned}
\nabla Y_t^{-1} - \mu &= e_t \quad \quad \quad &(1) \\
\nabla Y_t^{-1} - \mu &= e_t + \phi \left(\nabla Y_{t-1}^{-1} - \mu \right) \quad \quad \quad &(2)
\end{aligned}
$$

* NPA

$$
\begin{aligned}
\nabla Y_t^{-1} - \mu &= e_t \quad \quad \quad &(3) \\
\nabla Y_t^{-1} - \mu &= e_t - \theta e_{t-1} \quad \quad \quad &(4)
\end{aligned}
$$

* NDI

$$
\begin{aligned}
\nabla lnY_t - \mu &= e_t \quad \quad \quad &(5) \\
\nabla lnY_t - \mu &= e_t - \theta e_{t-1} \quad \quad \quad &(6) \\
\nabla lnY_t - \mu &= e_t + \phi \left(\nabla lnY_{t-1} - \mu \right) \quad \quad \quad &(7)
\end{aligned}
$$

* CTI

$$
\begin{aligned}
\nabla Y_t - \mu &= e_t \quad \quad \quad &(8) \\
\nabla Y_t - \mu &= e_t - \theta e_{t-3} \quad \quad \quad &(9)
\end{aligned}
$$

##Estimation des paramètres des différents modèles et tests de résidus
```{r no2.3, echo=F}
# assignation de l'arima pour chaque modele
naadc000 <- arima(diff(naadc^-1), order = c(0,0,0), method = "ML")
naadc100 <- arima(diff(naadc^-1), order = c(1,0,0), method = "ML")

npa000 <- arima(diff(npa^-1), order = c(0,0,0), method = "ML")
npa001 <- arima(diff(npa^-1), order = c(0,0,1), method = "ML")

ndi000 <- arima(diff(log(ndi)), order = c(0,0,0), method = "ML")
ndi001 <- arima(diff(log(ndi)), order = c(0,0,1), method = "ML")
ndi100 <- arima(diff(log(ndi)), order = c(1,0,0), method = "ML")

cti000 <- arima(diff(cti), order = c(0,0,0), method = "ML")
cti003 <- arima(diff(cti), order = c(0,0,3), fixed = c(0, 0, NA, NA), method = "ML")
```

Il faut maintenant estimer les paramètres de nos différents modèles. Les tableaux ci-dessous fournissent l'information nécessaire

$$
\begin{tabular}{| l c |c|c| r |}
  \hline
  Modèle pour NAADC & Paramètres & Valeurs estimés & AIC & Logarithme du max \\
  & & & &  de vraisemblance \\ \hline \hline
  ARIMA(0,1,0) & $\mu$ & $`r round(naadc000$coef[1],4)`$ & $`r round(naadc000$aic,0)`$ & $`r round(naadc000$loglik,1)`$\\    \hline
  ARIMA(1,1,0) & $\mu$ & $`r round(naadc100$coef[2],4)`$ & $`r round(naadc100$aic,0)`$ & $`r round(naadc100$loglik,1)`$\\
    & $\phi$ & $`r round(naadc100$coef[1],4)`$&  &\\
  \hline
 \end{tabular}
$$

$$
\begin{tabular}{| l c |c|c| r |}
  \hline
  Modèle pour NPA & Paramètres & Valeurs estimés & AIC & Logarithme du max \\
  & & & &  de vraisemblance \\ \hline \hline
  ARIMA(0,1,0) & $\mu$ & $`r round(npa000$coef[1],4)`$ & $`r round(npa000$aic,0)`$ & $`r round(npa000$loglik,1)`$\\    \hline
  ARIMA(0,0,1) & $\mu$ & $`r round(npa001$coef[2],4)`$ & $`r round(npa001$aic,0)`$ & $`r round(npa001$loglik,1)`$\\
    & $\theta$ & $`r round(npa001$coef[1],4)`$&  &\\
  \hline
 \end{tabular}
$$

$$
\begin{tabular}{| l c |c|c| r |}
  \hline
  Modèle pour NDI & Paramètres & Valeurs estimés & AIC & Logarithme du max \\
  & & & &  de vraisemblance \\ \hline \hline
  ARIMA(0,1,0) & $\mu$ & $`r round(ndi000$coef[1],4)`$ & $`r round(ndi000$aic,0)`$ & $`r round(ndi000$loglik,1)`$\\        \hline
  ARIMA(0,1,1) & $\mu$ & $`r round(ndi001$coef[2],4)`$ & $`r round(ndi001$aic,0)`$ & $`r round(ndi001$loglik,1)`$\\
    & $\theta$ & $`r round(ndi001$coef[1],4)`$&  &\\
  \hline
  ARIMA(1,1,0) & $\mu$ & $`r round(ndi100$coef[2],4)`$ & $`r round(ndi100$aic,0)`$ & $`r round(ndi100$loglik,1)`$\\
    & $\phi$ & $`r round(ndi100$coef[1],4)`$&  &\\
  \hline
 \end{tabular}
$$

$$
\begin{tabular}{| l c |c|c| r |}
  \hline
  Modèle pour CTI & Paramètres & Valeurs estimés & AIC & Logarithme du max \\
  & & & &  de vraisemblance \\ \hline \hline
  ARIMA(0,1,0) & $\mu$ & $`r cti003$coef[1]`$ & $`r round(cti000$aic,0)`$ & $`r round(cti000$loglik,1)`$\\    \hline
  ARIMA(0,0,3) & $\mu$ & $`r cti003$coef[4]`$ & $`r round(cti003$aic,0)`$ & $`r round(cti003$loglik,1)`$\\
    & $\theta$ & $`r cti003$coef[3]`$&  &\\
  \hline
 \end{tabular}
$$

On peut maintenant faire l'analyse de nos résidus pour faire notre choix de modèle. Puisque nous avons plusieurs modèles par série chronologique, nous allons séparer les graphiques en 4 sections, i.e. pour les 4 modèles transformés différenciés.

### NAADC
```{r no2.4, echo=F}
naadc000.sw <- shapiro.test(rstandard(naadc000))
naadc100.sw <- shapiro.test(rstandard(naadc100))

naadc000.rt <- runs(residuals(naadc000))$pvalue
naadc100.rt <- runs(residuals(naadc100))$pvalue

naadc000.lb <- Box.test(residuals(naadc000), lag = 10, type = "Ljung-Box", fitdf = 1)
naadc100.lb <- Box.test(residuals(naadc100), lag = 10, type = "Ljung-Box", fitdf = 1)
```

Les résidus standardisés des deux modèles que nous avons gardés apparaîssent dans le graphique ci-dessous. On remarque que les courbes sont plutôt superposées. Puisque les valeurs des résidus standardisés sont généralement comprises dans une fourchette de $\pm 2$, l'hypothèse de normalité semble adéquate.

```{r fig2.8, echo=F, fig.height=3}
# graphiques résidus standardisés
plot(rstandard(naadc000), ylab = "résidus standardisés", type = "l", col = "blue")
lines(rstandard(naadc100), col = "red")
```

On regarde maintenant les histogrammes des résidus. Les deux histogrammes ne semblent pas tracer la cloche de la loi normale tel que nous la connaissons.

```{r fig2.9, echo=F, out.width='.49\\linewidth', fig.height=4, fig.width=7, fig.align="left", fig.show='hold',fig.align='center', warning=F}
# histogrammes
hist(rstandard(naadc000), xlab='résidus standardisés',ylab='fréquence', main='ARMA(0,0)')
hist(rstandard(naadc100), xlab='résidus standardisés',ylab='fréquence', main='ARMA(1,0)')
```

On poursuit notre étude des résidus avec le graphique QQ. On remarque que les résidus observés ne semblent pas être similaire à leur valeur théorique en cas de normalité.

```{r fig2.10, eval=T, echo=F, fig.height=3.3, fig.width=2.3, fig.show='hold', fig.align='center', warning=F, error=F}
# QQ Plot
qqnorm(rstandard(naadc000), main='ARMA(0,0)',ylab='Quantiles observés',xlab='Quantiles théoriques',cex=0.2)
qqnorm(rstandard(naadc100),main='ARMA(1,0)',xlab='Quantiles théoriques', ylab='Quantiles observés',cex=0.2)
```

On effectue maintenant le test de Shapiro-Wilks pour départager nos modèles. Les p-values respectives du modèle ARMA(0,0) et ARMA(1,0) sont `r round(naadc000.sw$p.value,4)` et `r round(naadc100.sw$p.value,4)`. Puisque les p-values sont supérieures à 5%, nous gardons l'hypothèse nulle qui stipule que les résidus suivent une loi normale.

Par la suite, il faut effectuer le run-test pour déterminer si les résidus sont indépendants. Les p-values de nos modèles ARMA(0,0) et ARMA(1,0) sont `r round(naadc000.rt,4)` et `r round(naadc100.rt,4)`. Puisqu'elles sont nettement supérieures à 5%, on accepte l'hypothèse nulle qui stipule l'indépendance entre les résidus.

```{r fig110, eval=T, echo=F, fig.height=5, fig.width=3.5, fig.show='hold', fig.align='center', warning=F}
tsdiag(naadc000, gof=15, omit.inital=F)
tsdiag(naadc100, gof=15, omit.inital=F)
```

Ljung-Box nous dit pour les modèles ARMA(0,0) et ARMA(1,0) que leur p-value sont respectivement de `r round(naadc000.lb$p.value, 4)` et `r round(naadc100.lb$p.value, 4)`. Puisqu'elle sont supérieures à 5%, les modèles sont appropriés.

Puisque les résidus des 2 modèles semblent suivre une normale, que les log de max de vraisemblance sont identiques tout comme les AIC, on choisit de conserver le modèle ARMA(0,0), i.e. le bruit blanc parce qu'il est simple et que les graphiques ACF et PACF supportent cette hypothèse.

```{r no2.5, echo=F, fig.height=4}
naadc001 <- arima(diff(naadc^-1), order = c(0,0,1), method = "ML")
```

On regarde maintenant la question de la surparamétrisation. Dans notre cas, puisqu'on a déjà testé le modèle ARMA(1,0), il ne reste qu'à tester le modèle ARMA(0,1). Ce modèle semble adéquat avec un paramètre $\phi$ = 0.128972 et une moyenne $\mu$ = 2.988855e-07. Cependant, on reste avec notre hypothèse de départ et on suppose que le modèle final du NAADC est un bruit blanc, i.e. $\nabla Y_t^{-1} = e_t$.

### NPA
```{r npa1, echo=F}
#test augmente de Dickey-Fuller
npa000.sw <- shapiro.test(rstandard(npa000))
npa001.sw <- shapiro.test(rstandard(npa001))

npa000.rt <- runs(residuals(npa000))$pvalue
npa001.rt <- runs(residuals(npa001))$pvalue

npa000.lb <- Box.test(residuals(npa000), lag = 10, type = "Ljung-Box", fitdf = 1)
npa001.lb <- Box.test(residuals(npa001), lag = 10, type = "Ljung-Box", fitdf = 1)
```
On cherche alors à savoir si les résidus suivent des lois normales.

```{r npa2, echo=F, out.width='.49\\linewidth', fig.height=5, fig.width=7, fig.align="left", fig.show='hold',fig.align='center', warning=F}
plot(rstandard(npa000), ylab = "résidus standardisés", type = "l", col = "blue")
lines(rstandard(npa001), col = "red")
```
\textbf{Figure ??} Superposition des graphiques des résidus standardisés des modèles ARIMA(0,1,0) et ARIMA(0,1,1) du data NPA.\newline\newline

On remarque que les résidus standardisés sont inférieurs à 2 en valeurs absolues ce qui est souhaitable pour des lois normales centrées réduites.

```{r npa3, echo=F, out.width='.49\\linewidth', fig.height=5, fig.align="left", fig.show='hold',fig.align='center', warning=F}
# histogrammes
hist(rstandard(npa000), xlab='résidus standardisés',ylab='fréquence', main='ARMA(0,0)')
hist(rstandard(npa001), xlab='résidus standardisés',ylab='fréquence', main='ARMA(1,0)')
```
\textbf{Figure ??} Histogramme des résudies standardisés des modèles ARIMA(0,1,0) et ARIMA(0,1,1) du data NPA.\newline\newline

Les histogrammes des deux modèles à l'étude semblent difficilement tracer de loi normal tel qu'on remarque du graphique ci-dessus.
```{r npa4, eval=T, echo=F, fig.height=3.3, fig.width=2.3, fig.show='hold', fig.align='center'}
qqnorm(rstandard(naadc000), main='ARMA(0,0)',ylab='Quantiles observés',xlab='Quantiles théoriques',cex=0.2)
qqnorm(rstandard(naadc100),main='ARMA(1,0)',xlab='Quantiles théoriques', ylab='Quantiles observés',cex=0.2)
```
\textbf{Figure ??} graphique QQ des modèles ARIMA(0,1,0) et ARIMA(0,1,1) \newline\newline

Les graphiques QQ ne semblent pas tracer de belle droite ce qui fait douter de la fiabilité de l'hypothèse de normalité.\newline\newline 

Le test de Shapiro-Wilk ne permet toutefois pas de rejetter l'hyptohèse nulle de noramlité des résidus, ni dans le cas ARIMA(0,1,0), ni dans le cas du ARIMA(0,1,1) avec des  p-values respectivent de `r round(npa000.sw$p.value,3)` et `r round(npa001.sw$p.value,3)`. \newline\newline

On ne peut ici rejetter l'hypothèse nulle d'indépendance des résidus du run-test avec des p-values respectives de`r round(npa000.rt,3)` et `r round(npa001.rt,3)` pour les mêmes modèles que ceux testés précédemment.\newline\newline

```{r npa5, eval=T, echo=F, fig.height=5, fig.width=3.5, fig.show='hold', fig.align='center', warning=F}
tsdiag(ttaux011, gof=15, omit.inital=F)
tsdiag(ttaux110, gof=15, omit.inital=F)
```

Vient finalement le test de Ljung–Box qui a pour hypothèse nul que le modèle testé est approprié. On obtient, pour les modèles ARIMA(0,1,0) et ARIMA(0,0,1), les p-values respectives `r round(npa000.lb$p.value,3)` et `r round(npa001.lb$p.value,3)` qui viennent toutes deux justifiées leur modèles respectifs à un niveau de confiance arbitraire de 5%.\newline\newline

Puisque les deux modèles engendrent les mêmes conclusion pour tous les tests, on se fit aux graphiques de l'ACF et du PACF pour retenir le bruit blanc comme modèle approprié. On se penche alors sur la question de l'overfiting. Comme le modèle ARIMA(0,1,1) a déjà été testé, on se penche uniquement sur le cas du modèeles ARIMA(1,1,0) pour l'overfiting.\newline\newline

```{r npa6, echo=F}
npa100 <- arima(naadc^-1, order = c(1,1,0), method = "ML")
#test augmente de shapiro
npa100.sw <- shapiro.test(rstandard(npa100))

npa100.rt <- runs(residuals(npa100))$pvalue

npa100.lb <- Box.test(residuals(npa100), lag = 10, type = "Ljung-Box", fitdf = 1)
```

La sortie R de la fonction arima pour le modèle ARIMA(1,1,0) renvoit un coefficient autorégressif de 0.1592 ce qui ne tend pas significativement vers 0. Ce modèle est donc à considérer. Toutefois, le test de Shapiro-Wilks pour la normalité des résidus de ce modèle renvoit un valeur de p-value de 6.003%, qui est concluant pour un test à niveau de confiance de 5%, mais pas très fortement. Comme la normalité des résidus du modèle est plus concluante pour le modèele ARIMA(0,1,0) et que les graphiques de l'ACF et du PACF suggèrent très fortement ce modèles, c'est celui qui sera retenu.\newline\newline

### NDI
```{r no2.67, echo=F}
ndi000.sw <- shapiro.test(rstandard(ndi000))
ndi001.sw <- shapiro.test(rstandard(ndi001))
ndi100.sw <- shapiro.test(rstandard(ndi100))

ndi000.rt <- runs(residuals(ndi000))$pvalue
ndi001.rt <- runs(residuals(ndi001))$pvalue
ndi100.rt <- runs(residuals(ndi100))$pvalue

ndi000.lb <- Box.test(residuals(ndi000), lag = 10, type = "Ljung-Box", fitdf = 1)
ndi001.lb <- Box.test(residuals(ndi001), lag = 10, type = "Ljung-Box", fitdf = 1)
ndi100.lb <- Box.test(residuals(ndi100), lag = 10, type = "Ljung-Box", fitdf = 1)
```

Les résidus standardisés des trois modèles que nous avons gardés apparaîssent dans le graphique ci-dessous. On remarque que les courbes sont plutôt superposées. Les valeurs des résidus standardisés sont à quelques reprises à l'extérieur d'une fourchette de $\pm 2$, l'hypothèse de normalité semble plus ou moins convaicante.

```{r fig4.8, echo=F, fig.height=3}
# graphiques résidus standardisés
plot(rstandard(ndi000), ylab = "résidus standardisés", type = "l", col = "blue")
lines(rstandard(ndi001), col = "red")
lines(rstandard(ndi100), col = "green")
```

On regarde maintenant les histogrammes des résidus. Les trois histogrammes semblent tracer la cloche de la loi normale assez bien.

```{r fig4.9, echo=F, out.width='.49\\linewidth', fig.height=4, fig.width=7, fig.align="left", fig.show='hold',fig.align='center', warning=F}
# histogrammes
hist(rstandard(ndi000), xlab='résidus standardisés',ylab='fréquence', main='ARMA(0,0)')
hist(rstandard(ndi001), xlab='résidus standardisés',ylab='fréquence', main='ARMA(0,1)')
hist(rstandard(ndi100), xlab='résidus standardisés',ylab='fréquence', main='ARMA(1,0)')
```

On poursuit notre étude des résidus avec le graphique QQ. On remarque que les résidus observés sont sensiblement similaire à leur valeur théorique en cas de normalité.

```{r fig4.10, eval=T, echo=F, out.width='.49\\linewidth', fig.height=4, fig.width=7, fig.align="left", fig.show='hold',fig.align='center', warning=F}
# QQ Plot
qqnorm(rstandard(ndi000), main='ARMA(0,0)',ylab='Quantiles observés',xlab='Quantiles théoriques',cex=0.2)
qqnorm(rstandard(ndi001),main='ARMA(0,1)',xlab='Quantiles théoriques', ylab='Quantiles observés',cex=0.2)
qqnorm(rstandard(ndi100),main='ARMA(1,0)',xlab='Quantiles théoriques', ylab='Quantiles observés',cex=0.2)
```

On effectue maintenant le test de Shapiro-Wilks pour départager nos modèles. Les p-values respectives du modèle ARMA(0,0), ARMA(0,1) et ARMA(1,0) sont `r round(ndi000.sw$p.value,4)`, `r round(ndi001.sw$p.value,4)` et `r round(naadc100.sw$p.value,4)`. Puisque les p-values sont supérieures à 5%, nous gardons l'hypothèse nulle qui stipule que les résidus suivent une loi normale. Cependant, on remarque que le modèle ARMA(1,0) est tout prêt de refuser cette hypothèse. On est en droit de mettre en doute la force du test.

Par la suite, il faut effectuer le run-test pour déterminer si les résidus sont indépendants. Les p-values de nos modèles ARMA(0,0), ARMA(0,1) et ARMA(1,0) sont `r round(ndi000.rt,4)`, `r round(ndi001.rt,4)` et `r round(ndi100.rt,4)`. On remarque pour le modèle du bruit blanc (ARMA(0,0)) que la p-value est inférieure à 5%. Dans ce cas, on rejette l'hypothèse que les résidus sont indépendants. Cependant, pour le modèle autorégressif et moyenne mobile, puisque les p-values sont nettement supérieures à 5%, on accepte l'hypothèse nulle qui stipule l'indépendance entre les résidus.

```{r fig1135, eval=T, echo=F, fig.height=5, fig.width=3.5, fig.show='hold', fig.align='center', warning=F}
tsdiag(ndi000, gof=15, omit.inital=F)
tsdiag(ndi001, gof=15, omit.inital=F)
tsdiag(ndi100, gof=15, omit.inital=F)
```

Ljung-Box nous dit pour les modèles ARMA(0,0), ARMA(0,1) et ARMA(1,0) que leur p-value sont respectivement de `r round(ndi000.lb$p.value, 4)`, `r round(ndi001.lb$p.value, 4)` et `r round(ndi100.lb$p.value, 4)`. Puisqu'elle sont supérieures à 5%, les modèles sont appropriés.

Nous avons donc écarté l'hypothèse de modèle basé uniquement sur le bruit blanc. De plus, puisque le graphique ACF semble montrer une forme autorégressive, et que le BIC supporte cette hypothèse, et que l'histogramme des résidus du ARMA(1,0) est plus représentatif d'une loi normale que l'ARMA(0,1), nous décidons de garder le modèle ARMA(1,0).

```{r no26.32, echo=F, fig.height=4}
#surparamétrisation
ndi200 <- arima(log(ndi), order = c(2,1,0), method = "ML")
ndi101 <- arima(log(ndi), order = c(1,1,1), method = "ML")

ndi101.sw <- shapiro.test(rstandard(ndi101))
ndi101.rt <- runs(residuals(ndi101))$pvalue
```

On regarde maintenant la question de la surparamétrisation.  On teste le modèle ARMA(2,0) et ARMA(1,1).

Pour le modèle ARMA(2,0), les coefficients $\phi_1$ et $\phi_2$ sont respectivement de `r round(ndi200$coef[1], 4)` et `r round(ndi200$coef[2], 4)`. Puisque le deuxième paramètre du modèle ARMA(2,0) n'est pas significativement différent de 0 (selon les intervalles de confiance), il est jugé de refuser le modèle ARMA(2,0) et garder notre modèle initial.

Pour ce qui est du modèle ARMA(1,1), la partie autorégressive ne semble pas convaincante puisque le coefficient $\phi$ tend vers 0. Il penche donc plus vers une moyenne mobile MA(1) qui est un des modèles que nous avions rejeté initialement. Donc, nous préférerons garder une fois de plus le modèle ARMA(1,0).

### CTI
```{r no2.8, echo=F}
cti000.sw <- shapiro.test(rstandard(cti000))
cti003.sw <- shapiro.test(rstandard(cti003))

cti000.rt <- runs(residuals(cti000))$pvalue
cti003.rt <- runs(residuals(cti003))$pvalue

cti000.lb <- Box.test(residuals(cti000), lag = 10, type = "Ljung-Box", fitdf = 1)
cti003.lb <- Box.test(residuals(cti003), lag = 10, type = "Ljung-Box", fitdf = 1)
```

Les résidus standardisés des deux modèles que nous avons gardés apparaîssent dans le graphique ci-dessous. On remarque que les courbes sont plutôt superposées. Puisque les valeurs des résidus standardisés sont généralement comprises dans une fourchette de $\pm 2$, l'hypothèse de normalité semble adéquate.

```{r fig2.12, echo=F, fig.height=3}
# graphiques résidus standardisés
plot(rstandard(cti000), ylab = "résidus standardisés", type = "l", col = "blue")
lines(rstandard(cti003), col = "red")
```

On regarde maintenant les histogrammes des résidus. Les deux histogrammes ne semblent pas tracer la cloche de la loi normale tel que nous la connaissons.

```{r fig2.13, echo=F, out.width='.49\\linewidth', fig.height=5, fig.align="left", fig.show='hold',fig.align='center', warning=F}
# histogrammes
hist(rstandard(cti000), xlab='résidus standardisés',ylab='fréquence', main='ARMA(0,0)')
hist(rstandard(cti003), xlab='résidus standardisés',ylab='fréquence', main='ARMA(1,0)')
```

On poursuit notre étude des résidus avec le graphique QQ. On remarque que les résidus observés ne semblent pas être similaire à leur valeur théorique en cas de normalité.

```{r fig2.14, eval=T, echo=F, fig.height=3.3, fig.width=2.3, fig.show='hold', fig.align='center'}
qqnorm(rstandard(cti000), main='ARMA(0,0)',ylab='Quantiles observés',xlab='Quantiles théoriques',cex=0.2)
qqnorm(rstandard(cti003),main='ARMA(0,3)',xlab='Quantiles théoriques', ylab='Quantiles observés',cex=0.2)
```

On effectue maintenant le test de Shapiro-Wilks pour départager nos modèles. Les p-values respectives du modèle ARMA(0,0) et ARMA(0,3) sont `r round(cti000.sw$p.value,4)` et `r round(cti003.sw$p.value,4)`. Puisque les p-values sont supérieures à 5%, nous gardons l'hypothèse nulle qui stipule que les résidus suivent une loi normale.

Par la suite, il faut effectuer le run-test pour déterminer si les résidus sont indépendants. Les p-values de nos modèles ARMA(0,0) et ARMA(0,3) sont `r round(cti000.rt,4)` et `r round(cti003.rt,4)`. Puisqu'elles sont nettement supérieures à 5%, on accepte l'hypothèse nulle qui stipule l'indépendance entre les résidus.

```{r fig114, eval=T, echo=F, fig.height=5, fig.width=3.5, fig.show='hold', fig.align='center', warning=F}
tsdiag(cti000, gof=15, omit.inital=F)
tsdiag(cti003, gof=15, omit.inital=F)
```

Ljung-Box nous dit pour les modèles ARMA(0,0) et ARMA(0,3) que leur p-value sont respectivement de `r round(cti000.lb$p.value, 4)` et `r round(cti003.lb$p.value, 4)`. Puisqu'elle sont supérieures à 5%, les modèles sont appropriés.

Puisque les résidus des 2 modèles semblent suivre une normale, que les log de max de vraisemblance sont identiques tout comme les AIC, on choisit de conserver le modèle ARMA(0,0), i.e. le bruit blanc parce qu'il est simple et que les graphiques ACF et PACF supportent cette hypothèse.

```{r no2.9, echo=F}
cti001 <- arima(diff(cti), order = c(0,0,1), method = "ML")
cti100 <- arima(diff(cti), order = c(1,0,0), method = "ML")

cti100.sw <- shapiro.test(rstandard(cti100))
cti001.sw <- shapiro.test(rstandard(cti001))
```

On regarde maintenant la question de la surparamétrisation. Dans notre cas, on doit tester le modèle ARMA(1,0) et ARMA(0,1). Les p-values respectives du modèle ARMA(1,0) et ARMA(0,1) sont `r round(cti100.sw$p.value,4)` et `r round(cti001.sw$p.value,4)`. Les résidus suivent bien une loi normale, mais ça reste que les graphiques ACF et PACF appuient le fait qu'il serait préférable de garder le modèle du bruit blanc, i.e. $\nabla Y_t = e_t$.

##Prédictions
### NAADC
```{r no2.11, echo=F}
# Prédictions avec bornes
naadc000 <- arima(naadc^-1, order = c(0,1,0), method = "ML")
naadc000.pred <- predict(naadc000, n.ahead = 5)
naadc000.inf <- naadc000.pred$pred - qnorm(1-0.025)*naadc000.pred$se
naadc000.sup <- naadc000.pred$pred + qnorm(1-0.025)*naadc000.pred$se

# Modèle final
naadc000.pred.reel <- naadc000.pred$pred^-1
naadc000.inf.reel <- naadc000.inf^-1
naadc000.sup.reel <- naadc000.sup^-1
```

On rappelle que notre modèle final est $\nabla Y_t^{-1} = e_t$. Les prédictions ainsi que les bornes de nos prédictions du modèle NAADC modifiés et différés sont affichés ci-dessous

```{r fig2.11, eval=T, echo=F, fig.height=3, fig.show='hold', fig.align='center'}
plot(naadc000, n.ahead=5, ylab='Fréquence^-1', xlab='Années', cex=0.5)
```

Donc, les valeurs sont dans le tableau suivant

| Année   | $Y_{t+l}$  | Borne inf.  | Borne sup.  |
|:---:|:---:|:---:|:---:|
| 2016  | 27917  | $`r round(naadc000.sup.reel[1], 4)`$  | $`r round(naadc000.inf.reel[1], 4)`$  |
| 2017  | 27917  | $`r round(naadc000.sup.reel[2], 4)`$  | $`r round(naadc000.inf.reel[2], 4)`$  |
| 2018  | 27917  | $`r round(naadc000.sup.reel[3], 4)`$  | $`r round(naadc000.inf.reel[3], 4)`$  |
| 2019  | 27917  | $`r round(naadc000.sup.reel[4], 4)`$  | $`r round(naadc000.inf.reel[4], 4)`$  |
| 2020  | 27917  | $`r round(naadc000.sup.reel[5], 4)`$  | $`r round(naadc000.inf.reel[5], 4)`$  |

### NPA
```{r npa60, echo=F}
npa000 <- arima(npa^-1, order = c(0,1,0), method = "ML")
npa000.pred<-predict(npa000, n.ahead=5)
npa000.inf<-npa000.pred$pred-qnorm(0.975)*npa000.pred$se
npa000.sup<-npa000.pred$pred+qnorm(0.975)*npa000.pred$se
#modele final
npa.pred<-round(npa000.pred$pred^-1,4)
npa.sup<-round(npa000.inf^-1, 4)
npa.inf<-round(npa000.sup^-1, 4)
```

On rappelle que notre modèle final est $\nabla ln Y_t = e_t + \phi \nabla ln Y_{t-1}$. Les prédictions ainsi que les bornes de nos prédictions du modèle NPA modifiés et différés sont affichés ci-dessous

```{r npa7, eval=T, echo=F,fig.height=3.2, fig.width=7.5, warning=F}
plot(npa000, n.ahead=5, ylab='NPA^(-1)', xlab='Années', cex=0.5)
```
\textbf{Figure ??} Projection sur 5 ans du nombre d'accidents avec personnes accidentés (NPA).\newline\newline

On remarque que l'intervalle de confiance augmentes avec le temps ce qui est dû à la variance croissante. On obtient ainsi les prédictions suivantes.

Donc, les valeurs sont dans le tableau suivant

| Année   | $Y_{t+l}$  | Borne inf.  | Borne sup.  |
|:---:|:---:|:---:|:---:|
| 2016  | 37351  | $`r round(npa.inf[1], 4)`$  | $`r round(npa.sup[1], 4)`$  |
| 2017  | 37351  | $`r round(npa.inf[2], 4)`$  | $`r round(npa.sup[2], 4)`$  |
| 2018  | 37351  | $`r round(npa.inf[3], 4)`$  | $`r round(npa.sup[3], 4)`$  |
| 2019  | 37351  | $`r round(npa.inf[4], 4)`$  | $`r round(npa.sup[4], 4)`$  |
| 2020  | 37351  | $`r round(npa.inf[5], 4)`$  | $`r round(npa.sup[5], 4)`$  |

### NDI
```{r ndi60, echo=F}
ndi100 <- arima(log(ndi), order = c(1,1,0), method = "ML")
ndi100.pred<-predict(ndi100, n.ahead=5)
ndi100.inf<-ndi100.pred$pred-qnorm(0.975)*ndi100.pred$se
ndi100.sup<-ndi100.pred$pred+qnorm(0.975)*ndi100.pred$se

#modele final
ndi.pred<-round(exp(ndi100.pred$pred+0.5*ndi100.pred$se^2),4)
ndi.inf<-round(exp(ndi100.inf), 4)
ndi.sup<-round(exp(ndi100.sup), 4)
```

On rappelle que notre modèle final est $\nabla Y_t^{-1} = e_t$. Les prédictions ainsi que les bornes de nos prédictions du modèle NPA modifiés et différés sont affichés ci-dessous

```{r ndi7, eval=T, echo=F,fig.height=3.2, fig.width=7.5, warning=F}
plot(ndi100, n.ahead=5, ylab='log(ndi)', xlab='Années', cex=0.5)
```
\textbf{Figure ??} Projection sur 5 ans du nombre d'accidents avec personnes accidentés (NPA).\newline\newline

On remarque que l'intervalle de confiance augmentes avec le temps ce qui est dû à la variance croissante. On obtient ainsi les prédictions suivantes.

| Année   | $Y_{t+l}$  | Borne inf.  | Borne sup.  |
|:---:|:---:|:---:|:---:|
| 2016  | 19896.61  | $`r round(ndi.inf[1], 4)`$  | $`r round(ndi.sup[1], 4)`$  |
| 2017  | 20123.51  | $`r round(ndi.inf[2], 4)`$  | $`r round(ndi.sup[2], 4)`$  |
| 2018  | 20283.27  | $`r round(ndi.inf[3], 4)`$  | $`r round(ndi.sup[3], 4)`$  |
| 2019  | 20418.52  | $`r round(ndi.inf[4], 4)`$  | $`r round(ndi.sup[4], 4)`$  |
| 2020  | 20545.05  | $`r round(ndi.inf[5], 4)`$  | $`r round(ndi.sup[5], 4)`$  |


### CTI
```{r no2.12, echo=F}
# Prédictions avec bornes
cti000 <- arima(cti, order = c(0,1,0), method = "ML")
cti000.pred <- predict(cti000, n.ahead = 5)
cti000.inf <- cti000.pred$pred - qnorm(1-0.025)*cti000.pred$se
cti000.sup <- cti000.pred$pred + qnorm(1-0.025)*cti000.pred$se
```

On rappelle que notre modèle final est $\nabla Y_t = e_t$. Les prédictions ainsi que les bornes de nos prédictions du modèle CTI sont affichés ci-dessous

```{r fig2.17, eval=T, echo=F, fig.height=3, fig.show='hold', fig.align='center'}
plot(cti000, n.ahead=5, ylab='Coût', xlab='Années', cex=0.5)
```

\newpage

# Annexe
## Code




